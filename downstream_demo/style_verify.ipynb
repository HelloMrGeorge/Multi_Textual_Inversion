{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model_path = \"/root/annotated_textual_inversion/model/sd-v1-5\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "pipe.safety_checker = lambda images, clip_input: (images, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding\n",
    "\n",
    "token = \"<Monet>\"\n",
    "number = 1\n",
    "weight_dir=\"/root/annotated_textual_inversion/embeds/Monet-TI-2000\" # Monet-MTI-2-1200; Monet-ATI-1200; Monet-TI-2000\n",
    "name=\"learned_embeds\"\n",
    "\n",
    "# for MTI\n",
    "# token = load_MTI_embeds(pipe, model_path, token, number, weight_dir, name)\n",
    "\n",
    "# for TI\n",
    "pipe.load_textual_inversion(model_path, token, weight_name=os.path.join(weight_dir, f\"{name}.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "image_path = \"/root/annotated_textual_inversion/data/style/palace.jpg\"\n",
    "init_image = Image.open(image_path).convert(\"RGB\")\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "prompt = f\"{token} style, a palace of Forbidden City\" # a road lined with pink flowers; waterfall; a palace of Forbidden City\n",
    "output_dir = f\"/root/annotated_textual_inversion/output/samples\"\n",
    "num_samples = 10\n",
    "step = 50\n",
    "guidance_scale = 8\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "pipe.set_progress_bar_config(disable=False)\n",
    "\n",
    "sample = pipe(prompt=prompt, image=init_image, num_inference_steps=step, guidance_scale=guidance_scale, generator=generator).images\n",
    "sample[0].save(f\"{output_dir}/sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"generating samples ...\")\n",
    "for i in trange(num_samples):\n",
    "    sample = pipe(prompt=prompt, image=init_image, num_inference_steps=step, guidance_scale=guidance_scale, generator=generator).images\n",
    "    sample[0].save(f\"{output_dir}/sample-{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ldm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from utils import *\n",
    "from transformers import AutoProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"./model/clip-vit-large-patch14/\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"./model/clip-vit-large-patch14/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5346982479095459"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate image pair\n",
    "\n",
    "target_name = \"waterfall-512.jpg\"\n",
    "sample_name = \"TI.png\" # ATI.png; MTI.png; TI.png\n",
    "dir = \"waterfall\"\n",
    "\n",
    "target = f\"./data/style/{dir}/{target_name}\"\n",
    "sample = f\"./data/style/{dir}/{sample_name}\"\n",
    "\n",
    "clip_validate_image_pair(model, processor, sample, target, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2521168887615204"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate text pair\n",
    "\n",
    "sample_name = \"MTI.png\" # ATI.png; MTI.png; TI.png\n",
    "dir = \"waterfall\"\n",
    "\n",
    "text = \"a painting in the style of Monet\"\n",
    "sample = f\"./data/style/{dir}/{sample_name}\"\n",
    "\n",
    "clip_validate_text_image_pair(model, processor, text, sample, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
